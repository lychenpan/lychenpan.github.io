title: 单卡同时运行多个深度学习模型推理
author: lychenpan
tags:
  - multiprocessing
  - ''
  - inference
  - deep learning
categories:
  - Backend
date: 2020-03-30 21:20:00
---
##### 应用场景：
当多个深度学习模型（基于pytorch或者tensorflow）需要部署在一台服务器或者一张GPU显卡时，可以通过multiprocessing实现模型推理的并行，从而节省整体运行时间。
##### 方法：
生产者消费者模型，然后将不同的模型放在不同的 **multiprocessing.Process** 中进行处理； 采用JoinableQueue 进行不同进程之间的数据同步。
###### 踩过的坑：
1. 将模型的初始化放在主进程，然后将模型的inference，postprocess放在子进程是不可行的； 系统会报 recursion exceed maximum.  